# -*- coding: utf-8 -*-
"""casa0018-assessment-v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hlXXgmIqtr6_Xi8As6-9tsjBmdwLoKEd

# CASA0018 Assessment
version 2 -- Update 23/Mar/2022

Minghao ZHANG

This is the notebook for the assessment of the course CASA0018: Deep Learning for Sensor Networks.
The Assessment aims to build an individual project, which implements machine learning locally on a device.

The topic is Rock-Paper-Scissors Recognition based on the camera. The three different gestures are identified in real time.

Convolutional Neural Network (CNN) is used.

## Import dependencies

This section is to import relevant dependencies
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x

import os
import zipfile
import random
import tensorflow as tf
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt
import numpy as np
import platform
import datetime
import math

from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from shutil import copyfile

"""## Load Tensorflow Dataset

The dataset [rock_paper_scissors](https://www.tensorflow.org/datasets/catalog/rock_paper_scissors) contains images of hands playing rock, paper and scissor game. 

The soruce is from [Tensorflow Datasets](https://www.tensorflow.org/datasets/catalog/overview)

Source code: `tfds.image_classification.RockPaperScissors`

**Load Tensorflow dataset - rock_paper_scissors**
"""

DATASET_NAME = 'rock_paper_scissors'

# Load dataset
dataset, info = tfds.load(DATASET_NAME, as_supervised=True, with_info=True)

"""**Look into dataset info**

The datasets contains 2892 images with resolution 300*300 in RGB colour space and three different labels 
"""

info

"""**Check dataset labels**

label -- rock, paper, scissors
"""

# Check dataset labels
num_classes = info.features["label"].num_classes  # number of classes
get_class_name = info.features["label"].int2str      # class names

print(num_classes)        # 3
print(get_class_name(0)); # rock
print(get_class_name(1)); # paper
print(get_class_name(2)); # scissors

"""**Slice the dataset to three sets (Train, validation and test)**

Number of images:
+ Train set: 2142
+ Valid set: 372
+ Test set: 378

The ratio is about 75%, 12.5%, 12.5%

"""

train_set_raw, valid_set_raw, test_set_raw = tfds.load(DATASET_NAME, 
                                           split=["train[:85%]", "train[85%:]", "test"],
                                           as_supervised=True)
TRAIN_SIZE = len(train_set_raw)
VALID_SIZE = len(valid_set_raw)
TEST_SIZE = len(test_set_raw)

print("Train set size: ", TRAIN_SIZE)   # Train set size:  2142
print("Valid set size: ", VALID_SIZE)   # Valid set size:  378
print("Test set size: ", TEST_SIZE)     # Test set size:  372

"""## Explore the Dataset

**Preview the dataset**
"""

# define dataset preview
def preview_dataset(dataset):
  plt.figure(figsize=(12, 8))
  plot_index = 0
  for features in dataset.take(8):
    (image, label) = features
    plot_index += 1
    plt.subplot(2, 4, plot_index)
    # plt.axis('Off')
    label = get_class_name(label.numpy())
    plt.title('Label: %s' % label)
    plt.imshow(image.numpy())

"""**Preview the train set**"""

preview_dataset(train_set_raw)

"""**Preview the validation set**"""

preview_dataset(valid_set_raw)

"""**Preview the test set**"""

preview_dataset(test_set_raw)

"""**Explore the image format**"""

# Explore the image format
(image_sample, image_label) = list(train_set_raw.take(1))[0]
print('Label:', image_label.numpy(), '\n')
print('Image shape:', image_sample.numpy().shape, '\n')
print(image_sample.numpy())

"""## Pre-processing the dataset

Resize, transform, shuffle and batch images

#### Image Resize

**Set expected image size**

resize images from 300x300 to 150x150
"""

IMG_SIZE_ORG = info.features['image'].shape[0]
IMG_SHAPE_ORG = info.features['image'].shape

IMG_SIZE_MOD = IMG_SIZE_ORG // 2
IMG_SHAPE_MOD = (IMG_SIZE_MOD, IMG_SIZE_MOD, IMG_SHAPE_ORG[2])

# Here we may switch between bigger or smaller image sized that we will train our model on.
IMG_SIZE = IMG_SIZE_MOD
IMG_SHAPE = IMG_SHAPE_MOD

print('Original image size:', IMG_SIZE_ORG)
print('Original image shape:', IMG_SHAPE_ORG)
print('Reduced image size:', IMG_SIZE_MOD)
print('Reduced image shape:', IMG_SHAPE_MOD)
print('Image size:', IMG_SIZE)
print('Image shape:', IMG_SHAPE)

"""**Reduce the image size**"""

def resize_image(img, label):
  img = tf.cast(img, tf.float32) # set color values to float
  img = img / 255.               # map color values to the range [0 1]
  img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])
  return img, label

train_set_resize = train_set_raw.map(resize_image)
valid_set_resize = valid_set_raw.map(resize_image)
test_set_resize = test_set_raw.map(resize_image)

"""**Check the image format after the size reduced**"""

# Check the image format
(image_sample, image_label) = list(train_set_resize.take(1))[0]
print('Label:', image_label.numpy(), '\n')
print('Image shape:', image_sample.numpy().shape, '\n')
print(image_sample.numpy())

"""**Check the dataset preview**"""

preview_dataset(train_set_resize)

"""#### Image Transform

**Define transformation**

The direction of various hands in the datasets are the same. To simulate the real situation, a series of image transform are defined and implemented, including colour, inversion, zoom, flip, rotate and grayscale.

Random function is applied to ensure randomness.
"""

# colour
def random_img_colour(img: tf.Tensor) -> tf.Tensor:
  img = tf.image.random_brightness(img, 0.06)
  img = tf.image.random_hue(img, max_delta=0.06)
  img = tf.image.random_saturation(img, lower=0.6, upper=1.4)
  img = tf.image.random_contrast(img, lower=0.75, upper=1)
  img = tf.clip_by_value(img, clip_value_min=0, clip_value_max=1)
  return img

# rotation (0/90/180/270 degrees) 
def random_img_rotate(img: tf.Tensor) -> tf.Tensor:
  # Rotate 0, 90, 180, 270 degrees
  return tf.image.rot90(
    img,
    tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32)
  )

# flip up, down, left and right
def random_img_flip(img: tf.Tensor) -> tf.Tensor:
  img = tf.image.random_flip_left_right(img)
  img = tf.image.random_flip_up_down(img)
  return img

# zoom
def random_img_zoom(img: tf.Tensor, zoom_min=0.8, zoom_max=1.0) -> tf.Tensor:
  img_width, img_height, img_colours = img.shape
  crop_size = (img_width, img_height)

  # crop range 1% to 20%
  scales = list(np.arange(zoom_min, zoom_max, 0.01))
  boxes = np.zeros((len(scales), 4))

  for i, scale in enumerate(scales):
    x1 = y1 = 0.5 - (0.5 * scale)
    x2 = y2 = 0.5 + (0.5 * scale)
    boxes[i] = [x1, y1, x2, y2]

  def img_crop(img):
    # crop parameters
    crops = tf.image.crop_and_resize(
      [img],
      boxes=boxes,
      box_indices=np.zeros(len(scales)),
      crop_size=crop_size
    )
    # random crop
    return crops[tf.random.uniform(shape=[], minval=0, maxval=len(scales), dtype=tf.int32)]

  # set 50% propability to apply crop
  choice = tf.random.uniform(shape=[], minval=0., maxval=1., dtype=tf.float32)
  return tf.cond(choice < 0.5, lambda: img, lambda: img_crop(img))

# colour inversion
def random_img_inversion(img: tf.Tensor) -> tf.Tensor:
  random = tf.random.uniform(shape=[], minval=0, maxval=1)
  if random > 0.5:
    img = tf.math.multiply(img, -1)
    img = tf.math.add(img, 1)
  return img

# ----- GRAYSCALE ---------------------------------
def img_to_grayscale(img: tf.Tensor) -> tf.Tensor:
  img = tf.image.rgb_to_grayscale(img)
  return img
# ----- GRAYSCALE ---------------------------------

# apply random image transform
def image_transform(img,label):
  img = random_img_colour(img)
  img = random_img_inversion(img)
  img = random_img_zoom(img)
  img = random_img_flip(img)
  img = random_img_rotate(img)
# ----- GRAYSCALE ---------------------------------
  img = img_to_grayscale(img)
# ----- GRAYSCALE ---------------------------------  
  return img, label

"""**Implement transformation to train, validationa and test sets**"""

train_set_transformed = train_set_resize.map(image_transform)
valid_set_transformed = valid_set_resize.map(image_transform)
test_set_transformed = test_set_resize.map(image_transform)

# Check the image format
(image_sample, image_label) = list(train_set_transformed.take(1))[0]
print('Label:', image_label.numpy(), '\n')
print('Image shape:', image_sample.numpy().shape, '\n')
print(image_sample.numpy())

"""**Check dataset after transformed**"""

# ----- GRAYSCALE ---------------------------------

# define dataset preview
def preview_dataset_grayscale(dataset):
  plt.figure(figsize=(12, 8))
  plot_index = 0
  for features in dataset.take(8):
    (image, label) = features
    plot_index += 1
    plt.subplot(2, 4, plot_index)
    label = get_class_name(label.numpy())
    plt.title('Label: %s' % label)
    plt.imshow(image.numpy()[:,:,0],cmap='gray')

# ----- GRAYSCALE ---------------------------------

# preview_dataset(train_set_transformed)
# preview_dataset(valid_set_transformed)
# preview_dataset(test_set_transformed)

# ----- GRAYSCALE ---------------------------------
preview_dataset_grayscale(train_set_transformed)
preview_dataset_grayscale(valid_set_transformed)
preview_dataset_grayscale(test_set_transformed)

"""#### Shuffling and batching

Shuffling the data to ensure the model not to learn something from the order. The data batching is to split the set to speed up the train.
"""

BATCH_SIZE = 16

train_set_shuffled = train_set_transformed.shuffle(buffer_size=TRAIN_SIZE)
train_set_shuffled = train_set_transformed.batch(batch_size=BATCH_SIZE)

# Prefetch will enable the input pipeline to asynchronously fetch batches while your model is training.
train_set_shuffled = train_set_shuffled.prefetch(
    buffer_size=tf.data.experimental.AUTOTUNE
)

valid_set_shuffled = valid_set_transformed.batch(BATCH_SIZE)
test_set_shuffled = test_set_transformed.batch(BATCH_SIZE)

print(train_set_shuffled)
print(valid_set_shuffled)
print(test_set_shuffled)

"""**Debug Batches**"""

# Debugging batches
batches = tfds.as_numpy(train_set_shuffled)
for batch in batches:
  image_batch, label_batch = batch
  print('Label batch shape:', label_batch.shape, '\n')
  print('Image batch shape:', image_batch.shape, '\n')
  print('Label batch:', label_batch, '\n')
    
  for batch_item_index in range(len(image_batch)):
    print('First batch image:', image_batch[batch_item_index], '\n')
# ---------------------------------------- GRAYSCALE ---------------------------------       
    plt.imshow(image_batch[batch_item_index][:,:,0],cmap='gray')
# ----------------------------------------- GRAYSCALE ---------------------------------
    # plt.imshow(image_batch[batch_item_index])
    plt.show()
    break
  break

"""## Build Model

This section is to create and compile the model
Create the model with four convolution layers. The output unit is set to 3 as three gestures to be classfied. 
"""

# ----- GRAYSCALE ---------------------------------
def create_model():
  model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu', input_shape = (150, 150, 1)), # shape 1 / for RGB using (150, 150, 3)
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(128, (3, 3), activation = 'relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(128, (3, 3), activation = 'relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(512, activation = 'relu'),
    tf.keras.layers.Dense(3, activation = 'softmax')
  ])

  adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
  rmsprop_optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)

  model.compile(
    optimizer=rmsprop_optimizer,
    loss=tf.keras.losses.sparse_categorical_crossentropy,
    metrics=['accuracy']
  )

  return model

model = create_model()

# ----- GRAYSCALE ---------------------------------

model.summary()

tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)

"""**Set steps and callbacks**"""

steps_per_epoch = TRAIN_SIZE // BATCH_SIZE
validation_steps = VALID_SIZE // BATCH_SIZE

print('steps_per_epoch:', steps_per_epoch)
print('validation_steps:', validation_steps)

!rm -rf tmp/checkpoints
!rm -rf logs

# Preparing callbacks.
os.makedirs('logs/fit', exist_ok=True)
tensorboard_log_dir = 'logs/fit/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')
tensorboard_callback = tf.keras.callbacks.TensorBoard(
    log_dir=tensorboard_log_dir,
    histogram_freq=1
)

# os.makedirs('tmp/checkpoints', exist_ok=True)
# checkpoint_path = 'tmp/checkpoints/weights.{epoch:02d}-{val_loss:.2f}.hdf5'
# checkpoint_dir = os.path.dirname(checkpoint_path)
# model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
#     filepath = checkpoint_path,
#     verbose = 1
# )

# os.makedirs('tmp/checkpoints', exist_ok=True)
checkpoint_path = "training_1/cp.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)

# Create a callback that saves the model's weights
cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                 save_weights_only=True,
                                                 verbose=1)

early_stopping_callback = tf.keras.callbacks.EarlyStopping(
    patience=5,
    monitor='val_accuracy'
    # monitor='val_loss'
)

"""## Train Model

This section is to train the model with checkpoints stored 
"""

training_history = model.fit(
    x=train_set_shuffled.repeat(),
    validation_data=valid_set_shuffled.repeat(),
    epochs=10,
    steps_per_epoch=steps_per_epoch,
    validation_steps=validation_steps,
    callbacks=[
        cp_callback,
        # early_stopping_callback,
        # tensorboard_callback
    ],
    verbose=1
)

os.listdir(checkpoint_dir)

"""**Plot training history**"""

def render_training_history(training_history):
    loss = training_history.history['loss']
    val_loss = training_history.history['val_loss']

    accuracy = training_history.history['accuracy']
    val_accuracy = training_history.history['val_accuracy']

    plt.figure(figsize=(15, 5))

    plt.subplot(1, 2, 1)
    plt.title('Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.plot(loss, label='Training set')
    plt.plot(val_loss, label='Validation set', linestyle='--')
    plt.legend()
    plt.grid(linestyle='--', linewidth=1, alpha=0.5)

    plt.subplot(1, 2, 2)
    plt.title('Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.plot(accuracy, label='Training set')
    plt.plot(val_accuracy, label='Validation set', linestyle='--')
    plt.legend()
    plt.grid(linestyle='--', linewidth=1, alpha=0.5)

    plt.show()

render_training_history(training_history)

"""## Test Model

Using pre-split test set to test the model (15%)
"""

test_loss, test_accuracy = model.evaluate(test_set_shuffled, verbose=1)

print('Test loss: ', test_loss)
print('Test accuracy: ', test_accuracy)

"""## Save and Restore Model

This section is to test model save and restore

#### from a certain checkpoint

A new model is created and tested by the test set. The result shows that the accuracy of the untrained model is about 33%.
"""

ckp_model = create_model()

ckp_loss, ckp_accuracy = ckp_model.evaluate(test_set_shuffled, verbose=1)

print('Test ckp_loss: ', ckp_loss)
print('Test ckp_accuracy: ', ckp_accuracy)

"""Load the model from the latest checkpoint, which is generated by the callback function."""

latest = tf.train.latest_checkpoint(checkpoint_dir)
latest

ckp_model.load_weights(latest)

ckp_loss, ckp_accuracy = ckp_model.evaluate(test_set_shuffled, verbose=1)

print('Test ckp_loss: ', ckp_loss)
print('Test ckp_accuracy: ', ckp_accuracy)

"""#### From final weights of trained model

Another new model is created and tested by the test set. The result shows that the accuracy of the untrained model is about 33%.
"""

model.save_weights('./checkpoints/my_checkpoint')
sv_model = create_model()
sv_loss, sv_accuracy = sv_model.evaluate(test_set_shuffled, verbose=1)

print('Test ckp_loss: ', sv_loss)
print('Test ckp_accuracy: ', sv_accuracy)

"""Load the model from the final checkpoint, which is generated by the save_weight() function to save the final weights of the trained model."""

# Restore the weights
sv_model.load_weights('./checkpoints/my_checkpoint')

sv_loss, sv_accuracy = sv_model.evaluate(test_set_shuffled, verbose=1)

print('Test ckp_loss: ', sv_loss)
print('Test ckp_accuracy: ', sv_accuracy)

"""#### From .h5 file
Save final trained model to .h5 file
"""

!pip install pyyaml h5py  # Required to save models in HDF5 format

# Save the entire model as a SavedModel.
!mkdir -p saved_model
model.save('saved_model/train_model_v1.h5')

"""Load model"""

new_model = tf.keras.models.load_model('saved_model/train_model_v1.h5')

# Show the model architecture
new_model.summary()

"""Test Model"""

test_loss, test_accuracy = new_model.evaluate(test_set_shuffled)

print('Test loss: ', test_loss)
print('Test accuracy: ', test_accuracy)

"""#### Save to Tensorflow Lite"""

# Define paths to model files
import os
MODELS_DIR = 'models/'
if not os.path.exists(MODELS_DIR):
    os.mkdir(MODELS_DIR)
MODEL_TF = MODELS_DIR + 'model'
MODEL_NO_QUANT_TFLITE = MODELS_DIR + 'model_no_quant.tflite'
MODEL_TFLITE = MODELS_DIR + 'model.tflite'
MODEL_TFLITE_MICRO = MODELS_DIR + 'model.cc'

# Save the model to disk
model.save(MODEL_TF)

"""Save tflite and quantization model"""

converter = tf.lite.TFLiteConverter.from_keras_model(model)  
tflite_model = converter.convert()  
with open('model_v1.tflite', 'wb') as f:  
    f.write(tflite_model)  

converter = tf.lite.TFLiteConverter.from_keras_model(model)  
converter.optimizations = [tf.lite.Optimize.DEFAULT]  
tflite_quant_model = converter.convert()  
with open('quant_model_v1.tflite', 'wb') as f:  
    f.write(tflite_quant_model)

"""## Upload Model to Google Drive
Upload the model weights to google drive for the further implements on the Raspberry Pi
"""

from google.colab import drive
drive.mount('/content/gdrive')
# !touch './checkpoints/my_checkpoint'
# !cp -r './checkpoints/my_checkpoint' /content/gdrive/ck1

"""## Upload Images to Test Performance

Images can be uploaded for recognition
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
from google.colab import files
from keras.preprocessing import image
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
# %matplotlib inline

uploaded = files.upload()

for fn in uploaded.keys():
  # predict images
  path = '/content/' + fn
  # img_source = image.load_img(path, target_size = (150, 150))
  img_source = image.load_img(path, color_mode = "grayscale", target_size = (150, 150))

  imgplot = plt.imshow(img_source)
  x = image.img_to_array(img_source)
  x = np.expand_dims(x, axis = 0)

  images = np.vstack([x])
  classes = model.predict(images, batch_size = 10)

  print(fn)
  # print(classes)
  if classes[0, 0] == 1:
    print('rock')
  elif classes[0, 1] == 1:
    print('paper')
  elif classes[0, 2] == 1:
    print('scissors')